# Multi-Touch Attribution API - Cursor Rules

You are an expert Python developer working on a Multi-Touch Attribution API built with FastAPI. This system processes marketing data to provide attribution insights with confidence scoring.

## Project Overview
- **Purpose**: Analyze marketing touchpoint data and apply attribution models
- **Tech Stack**: Python 3.8+, FastAPI, pandas, numpy, scikit-learn
- **Data Formats**: CSV, JSON, Parquet file support
- **Attribution Models**: First-touch, last-touch, linear, time-decay, position-based
- **Key Features**: Adaptive identity resolution, confidence scoring, business insights

## Code Standards

### Python Style
- Follow PEP 8 with 88-character line length (Black formatter)
- Use type hints for all functions: `def process_data(df: pd.DataFrame) -> AttributionResult:`
- Prefer explicit imports: `from typing import Dict, List, Optional`
- Use descriptive variable names: `attribution_results` not `attr_res`

### FastAPI Patterns
- Use Pydantic models for all request/response schemas
- Implement proper dependency injection
- Use HTTPException with structured error responses
- Follow async/await patterns for I/O operations

### Error Handling
Always structure errors like this:

```python
raise HTTPException(
    status_code=422,
    detail={
        "error": "validation_error",
        "message": "Detailed error description",
        "details": {"field": "customer_id", "issue": "missing_column"},
        "timestamp": datetime.utcnow().isoformat()
    }
)
```

### Data Processing

- Validate file formats before processing
- Use streaming for large files to manage memory
- Always include confidence scoring in results
- Implement proper cleanup for temporary files

### API Response Structure
All responses must follow the OpenAPI spec in SPECIFICATION.md:

```python
class AttributionResponse(BaseModel):
    results: AttributionResults
    metadata: AnalysisMetadata
    insights: Optional[List[BusinessInsight]] = None

class ChannelAttribution(BaseModel):
    credit: float = Field(..., ge=0.0, le=1.0)
    conversions: int = Field(..., ge=0)
    revenue: float = Field(..., ge=0.0)
    confidence: float = Field(..., ge=0.0, le=1.0)
```

### File Structure
src/
├── api/routes/          # FastAPI route handlers
├── core/attribution/    # Attribution model implementations
├── core/identity/       # Identity resolution logic
├── core/validation/     # Data validation
├── models/             # Pydantic schemas
├── utils/              # Utility functions
└── config/             # Configuration management
### Identity Resolution Logic
Always use this pattern for auto-selection:

```python
def select_linking_method(df: pd.DataFrame) -> LinkingMethod:
    if 'customer_id' in df.columns and df['customer_id'].notna().mean() > 0.8:
        return LinkingMethod.CUSTOMER_ID
    elif 'session_id' in df.columns and 'email' in df.columns:
        return LinkingMethod.SESSION_EMAIL
    elif 'email' in df.columns:
        return LinkingMethod.EMAIL_ONLY
    else:
        return LinkingMethod.AGGREGATE
```

### Attribution Models
Implement all models using strategy pattern:

```python
class AttributionModel(ABC):
    @abstractmethod
    def calculate_attribution(self, journey: CustomerJourney) -> Dict[str, float]:
        pass

class LinearAttributionModel(AttributionModel):
    def calculate_attribution(self, journey: CustomerJourney) -> Dict[str, float]:
        # Equal credit distribution
        credit_per_touchpoint = 1.0 / len(journey.touchpoints)
        return {tp.channel: credit_per_touchpoint for tp in journey.touchpoints}
```

### Testing Requirements

- Use pytest with async support
- Mock file uploads in tests
- Test all attribution models separately
- Include performance benchmarks
- Test error conditions thoroughly

### Confidence Scoring Requirements
Always calculate confidence based on:

```python
class ConfidenceCalculator:
    def calculate_confidence(self, data_quality: DataQuality, 
                           linking_accuracy: float) -> float:
        # Combine data quality metrics
        quality_score = (
            data_quality.completeness * 0.4 +
            data_quality.consistency * 0.3 +
            data_quality.freshness * 0.3
        )
        
        # Weight with linking accuracy
        return (quality_score * 0.7) + (linking_accuracy * 0.3)
```

### Key Principles

- Always validate data before processing
- Include confidence scores in all results
- Use proper HTTP status codes (400, 413, 422, 500)
- Follow the OpenAPI specification exactly
- Implement proper logging with correlation IDs
- Handle file cleanup and memory management
- Use dependency injection for testability

### Attribution Model Specifications

#### Linear Attribution

```python
def calculate_linear_attribution(touchpoints: List[Touchpoint]) -> Dict[str, float]:
    if not touchpoints:
        return {}
    
    credit_per_touchpoint = 1.0 / len(touchpoints)
    attribution = {}
    
    for touchpoint in touchpoints:
        if touchpoint.channel in attribution:
            attribution[touchpoint.channel] += credit_per_touchpoint
        else:
            attribution[touchpoint.channel] = credit_per_touchpoint
    
    return attribution
```

#### Time Decay Attribution

```python
def calculate_time_decay_attribution(touchpoints: List[Touchpoint], 
                                   half_life_days: float = 7.0) -> Dict[str, float]:
    if not touchpoints:
        return {}
    
    conversion_time = max(tp.timestamp for tp in touchpoints)
    total_weight = 0.0
    weights = {}
    
    for touchpoint in touchpoints:
        days_before_conversion = (conversion_time - touchpoint.timestamp).days
        weight = 2 ** (-days_before_conversion / half_life_days)
        weights[touchpoint] = weight
        total_weight += weight
    
    attribution = {}
    for touchpoint, weight in weights.items():
        credit = weight / total_weight
        if touchpoint.channel in attribution:
            attribution[touchpoint.channel] += credit
        else:
            attribution[touchpoint.channel] = credit
    
    return attribution
```

### Data Validation Patterns

```python
def validate_required_columns(df: pd.DataFrame) -> List[ValidationError]:
    errors = []
    required_columns = ['timestamp', 'channel', 'event_type']
    
    for column in required_columns:
        if column not in df.columns:
            errors.append(ValidationError(
                field=column,
                error_code="missing_required_column",
                message=f"Required column '{column}' is missing",
                suggestion=f"Add a '{column}' column to your data"
            ))
    
    return errors
```

### Performance Requirements

- Process files up to 10MB in <5 seconds
- Process files up to 100MB in <5 minutes
- Memory usage <2GB per request
- Support 10 concurrent requests per API key

When generating code, always consider:

- Data quality and edge cases
- Memory efficiency for large files
- Proper error handling and user feedback
- Confidence scoring methodology
- API specification compliance
- Mathematical accuracy of attribution models
- Proper timezone handling in datetime operations
- Security considerations for file uploads